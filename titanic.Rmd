---
title: "Logistic Regression - Titanic"
output: 
        html_document:
                toc: true
                toc_depth: 2
                toc_float: true
                number_sections: true
date: "`r Sys.Date()`"
editor_options: 
  chunk_output_type: inline
---

# Settings 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(caret)
library(ggplot2)
library(lattice)
library(doParallel)
library(kernlab)
ds_train <- read.csv(file = "data/train.csv")
ds_test <- read.csv(file = "data/test.csv")
subm_template <- read.csv(file = "data/gender_submission.csv")
# create full dataset
ds_test$Survived <- NA
ds_train$train <- T
ds_test$train <- F
ds_full <- rbind(ds_train, ds_test)
ds_full <- as.data.table(ds_full)
ds_full
# Embarked
ds_full[, table(Embarked, useNA = "always")]
ds_full[Embarked == "", Embarked := "S"]
ds_full[, table(Embarked, useNA = "always")]
# family size
ds_full[, family := SibSp + Parch + 1]
# identify adult vs child based on name
ds_full[, ad_ch := fcase(Name %like% "(Mr.|Mrs.|Dr\\.|Col\\.|Rev\\.|Mme\\.|Ms.)", "adult",
                         Name %like% "(Master)" & Parch > 0, "child",
                         Name %like% "Miss\\." & Parch > 0, "child",
                         family == 1, "adult",
                         Parch == 0, "adult",
                         Parch == 1 & SibSp == 1, "adult",
                         default = NA)]
ds_full[ad_ch == "adult"]
ds_full[ad_ch == "child"]
# Fare
ds_full[is.na(Fare)]
median_Fare_3Pclass <- ds_full[Pclass == 3, median(Fare, na.rm = T)]
ds_full[is.na(Fare), Fare := median_Fare_3Pclass]
remove(median_Fare_3Pclass)
# remove irrelevant
ds_full[, Ticket := NULL]
ds_full[, Cabin := NULL]
ds_full[, Name := NULL]

# impute Age
median_age_adult <- ds_full[ad_ch == "adult", median(Age, na.rm = T)]
median_age_child <- ds_full[ad_ch == "child", median(Age, na.rm = T)]
ds_full[is.na(Age), Age := fcase(ad_ch == "adult", median_age_adult,
                                 ad_ch == "child", median_age_child)]
remove(median_age_adult)
remove(median_age_child)
# factors
ds_full$Pclass <- as.factor(ds_full$Pclass)
ds_full$Sex <- as.factor(ds_full$Sex)
ds_full$Embarked <- as.factor(ds_full$Embarked)
ds_full$ad_ch <- as.factor(ds_full$ad_ch)
# check if variable Embarked seems relevant by means of logistic regression
model_glm <- glm(Survived ~ Sex + Age + Pclass + Embarked, 
              data = ds_full, 
              family = binomial)
summary(model_glm)
# Embarked seems in fact irrelevant, I remove it from the dataset
ds_full[, Embarked := NULL]
# split into training and test set again
ds_train <- ds_full[train == TRUE]
ds_train[, train := NULL]
ds_test <- ds_full[train == FALSE]
ds_test[, train := NULL]
remove(ds_full)
# convert variable Survived to factor
ds_train$Survived <- as.factor(ds_train$Survived)
# remove PassengerId 
ds_train[, PassengerId := NULL]
ds_train
```


# Review Imported Data Sets

```{r}
str(ds_train)
```

Submission template:
```{r}
str(subm_template)
```


```{r}
summary(ds_train)
```


# Clean Data

## Near Zero Variance Predictors

The identified near zero variance predictors are the following:

```{r}
# create a zero variance variable for demonstration purposes
ds_train$one <- 1
near_zero_vars <- nearZeroVar(ds_train)
ds_train[, ..near_zero_vars]
```


```{r}
if (length(near_zero_vars) > 0) {
        ds_train <- ds_train[, -c(..near_zero_vars)]
}
remove(near_zero_vars)
head(ds_train)
```


## Reduce Collinearity

**Collinearity** is the situation where a pair of predictor variables have a substantial correlation with each other. In general, there are good reasons to avoid data with highly correlated predictors as it can result in **highly unstable models** and **degraded predictive performance**.


## Box Plot

### Base R

```{r}
boxplot(Fare ~ Survived,
        data = ds_train,
  ylab = "Fare",
  main = "Fare ~ Survived"
)
```


### ggplot

```{r}
ggplot(ds_train) +
  aes(x = Survived, y = Fare) +
  geom_boxplot(fill = "#0c4c8a") +
  theme_minimal()
```


### lattice

```{r}
bwplot(Survived ~ Fare | Sex, ds_train)
```

## Scatter Plot

```{r}
pairs(ds_train,
      main = "Titanic - pairs(ds_train)",
      pch = 21,
      cex = 2,
      bg = c("red", "green3")[unclass(ds_train$Survived)])
```


```{r}
lattice::splom(ds_train,
               groups = ds_train$Survived)
```

#### ggpairs()

```{r}
library(GGally)
ggpairs(data = ds_train, progress = FALSE, ggplot2::aes(colour=Survived))
```


***
# Machine Learning 

Alternatives to try:

- Support Vector Machine
- Boosted Trees
- Random Forest


## Logistic Regression Base R

Start with all variables and exclude irrelevant ones.

```{r}
set.seed(997)

model_glm <- glm(Survived ~ ., data = ds_train, family = binomial)
summary(model_glm)
```


Confusion Matrix:

```{r}
prediction_glm_base <-
        predict(object = model_glm,
                newdata = ds_train,
                type = "response")

conf_matrix_glm_base <-
        confusionMatrix(as.factor(ifelse(prediction_glm_base < 0.5, 0, 1)),
                        ds_train$Survived)
accuracy_glm_base <- conf_matrix_glm_base$overall["Accuracy"]
conf_matrix_glm_base
```


## Logistic Regression (Caret) 

```{r}
model_glm_caret_preproc <- caret::train(
        Survived ~ .,
        data = ds_train,
        preProc = c("BoxCox", "center", "scale"),
        method = "glm"
)
model_glm_caret_preproc
```

```{r}
prediction_glm_caret <-
        predict(object = model_glm_caret_preproc,
                newdata = ds_train,
                type = "prob")

conf_matrix_glm_caret_preproc <-
        confusionMatrix(as.factor(ifelse(prediction_glm_caret[, 2] < 0.5, 0, 1)),
                        ds_train$Survived)
accuracy_glm_caret <- conf_matrix_glm_caret_preproc$overall["Accuracy"]
conf_matrix_glm_caret_preproc
```


## Random Forest

```{r}
# cl <- makePSOCKcluster(5)
# registerDoParallel(cl)
# train model
model_rf <- caret::train(Survived ~ .,
                          data = ds_train,
                          preProc = c("BoxCox", "center", "scale"),
                          method = "rf")
# stopCluster(cl)
model_rf
```

```{r}
plot(model_rf)
```

```{r}
caret::varImp(model_rf)
```

```{r}
prediction_rf <-
        predict(object = model_rf,
                newdata = ds_train,
                type = "prob")

conf_matrix_rf <-
        confusionMatrix(as.factor(ifelse(prediction_rf[, 2] < 0.5, 0, 1)),
                        ds_train$Survived)
accuracy_rf <- conf_matrix_rf$overall["Accuracy"]
conf_matrix_rf
```


## Support Vector Machine

```{r}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)


#Setup parameters for optimisation
params <-
        expand.grid(sigma = seq(0.05, 2, 0.05), C = seq(0.5, 2, 0.5))
#Train model with entire train data set
model_svmr <- train(Survived ~ .,
                    method = "svmRadial",
                    data = ds_train,
                    # preProcess = c("scale", "center"),
                    # trControl = control,
                    tuneGrid = params)

stopCluster(cl)

model_svmr$bestTune
# model_svmr$method
head(model_svmr$results)
```


Confusion matrix:

```{r}
prediction_svmr <-
        predict(object = model_svmr,
                newdata = ds_train)

conf_matrix_svmr <-
        confusionMatrix(prediction_svmr,
                        ds_train$Survived)
accuracy_svmr <- conf_matrix_svmr$overall["Accuracy"]
conf_matrix_rf
```


```{r}
#Predict Survival of Passengers in the test data
prediction_test_ds <- predict(model_rf, ds_test)
submission <- data.frame(PassengerID = ds_test$PassengerId,  Survived = prediction_test_ds)
```

```{r}
head(submission %>% head)
```

```{r}
table(submission$Survived)
```

```{r}
write.table(
        submission,
        file = "submission.csv",
        row.names = F,
        sep = ",",
        quote = FALSE
)
```

# Submission SVM
```{r}
prediction_test_ds_svm <- predict(model_svmr, ds_test)
submission_svm <- data.frame(PassengerID = ds_test$PassengerId,  Survived = prediction_test_ds_svm)
write.table(
        submission_svm,
        file = "submission.csv",
        row.names = F,
        sep = ",",
        quote = FALSE
)
```

